# -*- coding: utf-8 -*-
"""Spam filtering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A9JfR1deUg3b1bNdU2_sREc7sfzfr22x

Importing Dependencies
"""

import pandas as pd
import numpy as np
import string
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
nltk.download('stopwords')
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from collections import Counter
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import seaborn as sns
import warnings
warnings.filterwarnings('ignore', category=FutureWarning)

"""Exploring Data"""

# Load the CSV file
df = pd.read_csv('/content/spam.csv')

# Separate the input features (email text) and the target variable (spam/ham)
X = df['email_text']
y = df['spam']

# Create separate dataframes for spam and ham emails
spam_df = df[y == 'spam']
ham_df = df[y == 'ham']

# Check the number of emails in the dataset
num_emails = len(df)

# Count the number of spam and ham emails
num_spam = len(spam_df)
num_ham = len(ham_df)

# Print the results
print(f"Loaded {num_emails} emails from the dataset.")
print('Number of spam emails:', num_spam, " i.e. ", (round(((num_spam/(num_spam+num_ham))*100), 3)), "%")
print('Number of ham emails:', num_ham, " i.e. ", (round(((num_ham/(num_spam+num_ham))*100), 3)), "%")

"""NLP"""

# Remove stop words from the input features using NLTK
stop_words = set(stopwords.words('english'))
X = X.apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in stop_words]))

# Update spam and ham df
spam_df['email_text']=X
ham_df['email_text']=X
spam_df['email_text']=spam_df['email_text'].astype(str)
ham_df['email_text']=ham_df['email_text'].astype(str)

"""MNB Model Training"""

# Vectorize the input features using the CountVectorizer
vectorizer = CountVectorizer()
X_vect = vectorizer.fit_transform(X)

# Split the data into training and testing sets
test_size = float(input("\nPlease enter the test size (between 0 and 1): "))
random_state = 10
X_train, X_test, y_train, y_test = train_test_split(X_vect, y, test_size=test_size, random_state=random_state)

# Train the Multinomial Naive Bayes model
nb_model = MultinomialNB()
nb_model.fit(X_train, y_train)

"""MNB Model Testing"""

# Evaluate the model on the testing set
y_pred = nb_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)*100
precision = precision_score(y_test, y_pred, pos_label='spam')*100
recall = recall_score(y_test, y_pred, pos_label='spam')*100
f1 = f1_score(y_test, y_pred, pos_label='spam')*100

print("\n------------------- Results -------------------")
print("Accuracy:  ", round(accuracy, 3), "%")
print("Precision: ", round(precision, 3), "%")
print("Recall:    ", round(recall, 3), "%")
print("F1 Score:  ", round(f1, 3), "%")
print("------------------------------------------------")

"""Filtering System"""

# Prompt the user to input text to classify as spam or ham
while True:
    input_text = input("\nPlease enter some text to classify as spam or ham (or 'quit' to exit):\n")
    if input_text.lower() == 'quit':
        break
    else:
        # Preprocess the input text
        input_text = ' '.join([word for word in input_text.split() if word.lower() not in stop_words])
        input_vect = vectorizer.transform([input_text])

        # Predict the class of the input text
        prediction = nb_model.predict(input_vect)[0]

        # Add to csv
        new_row = {'spam': prediction, 'email_text': input_text}
        df = df.append(new_row, ignore_index=True)
        df.to_csv('/content/spam.csv', index=False)

        # Print the prediction
        if prediction == 'spam':
            print("\nThe input text is classified as SPAM.")
        else:
            print("\nThe input text is classified as HAM.")

"""Word Cloud"""

# Generate word cloud for spam emails
spam_text = ' '.join(spam_df['email_text'])
spam_wordcloud = WordCloud(width=800, height=500, random_state=42, max_font_size=100).generate(spam_text)
plt.figure(figsize=(10, 7))
plt.imshow(spam_wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud for Spam Emails')
plt.show()

# Generate word cloud for ham emails
ham_text = ' '.join(ham_df['email_text'])
ham_wordcloud = WordCloud(width=800, height=500, random_state=42, max_font_size=100).generate(ham_text)
plt.figure(figsize=(10, 7))
plt.imshow(ham_wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud for Ham Emails')
plt.show()

"""Common Words in Spam and Ham"""

# Combine the text from both dataframes into a single string
spam_text = ' '.join(spam_df['email_text'].tolist())
ham_text = ' '.join(ham_df['email_text'].tolist())
text = spam_text + ' ' + ham_text

# Remove any punctuation from the text
text = text.translate(str.maketrans('', '', string.punctuation))

# Split the text into words and count the frequency of each word
words = text.split()
word_counts = Counter(words)

# Create a new dataframe with the word frequencies and their percentage in both dataframes
total_words = len(spam_df['email_text'].tolist()) + len(ham_df['email_text'].tolist())
word_df = pd.DataFrame({'word': list(word_counts.keys()), 'count': list(word_counts.values())})
word_df['percentage'] = word_df['count'] / total_words * 100

# Filter the dataframe to only include words that appear in both the spam_df and ham_df
spam_words = spam_df['email_text'].str.split().tolist()
spam_words = [word for sublist in spam_words for word in sublist]
common_words = set(spam_words).intersection(set(word_df['word']))
common_word_df = word_df[word_df['word'].isin(common_words)]

# Sort the common word dataframe by frequency in descending order and select the top 15 words
common_word_df = common_word_df.sort_values(by='count', ascending=False).head(15)

# Create the stacked horizontal bar chart using matplotlib
fig, ax = plt.subplots(figsize=(10,5))
ax.barh(common_word_df['word'], common_word_df['percentage'], label='Common Words')
ax.legend()
ax.set_xlabel('Frequency (%)')
ax.set_ylabel('Words')
ax.set_title('Top 15 Most Common Words in Spam and Ham Emails')
plt.show()